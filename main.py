import os
import requests
import json
from flask import Flask, request, jsonify
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---
CHROMA_PATH = "./eyespy_memory_db"
OLLAMA_API = "http://localhost:11434/api/generate"

# 1. Kh·ªüi t·∫°o b·ªô nh·ªõ d√†i h·∫°n (T·ª± h·ªçc)
# S·ª≠ d·ª•ng model nh√∫ng mi·ªÖn ph√≠ ch·∫°y tr√™n CPU/GPU c·ªßa b·∫°n
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

app = Flask(__name__)

# 2. H√†m g·ªçi b·ªô n√£o Llama 3 (Ch·∫°y ho√†n to√†n tr√™n m√°y b·∫°n)
def call_local_ai(prompt, context):
    full_prompt = f"Ki·∫øn th·ª©c ƒë√£ h·ªçc: {context}\n\nC√¢u h·ªèi: {prompt}\nTr·∫£ l·ªùi nh∆∞ m·ªôt chuy√™n gia EyeSpyhub:"
    
    payload = {
        "model": "llama3",
        "prompt": full_prompt,
        "stream": False
    }
    response = requests.post(OLLAMA_API, json=payload)
    return response.json().get("response", "L·ªói k·∫øt n·ªëi b·ªô n√£o.")

# 3. API x·ª≠ l√Ω y√™u c·∫ßu v√† T·ª± h·ªçc
@app.route("/chat", methods=["POST"])
def chat():
    data = request.json
    user_query = data.get("query", "")

    # TRUY XU·∫§T: L·∫•y ki·∫øn th·ª©c c≈© li√™n quan t·ª´ b·ªô nh·ªõ
    docs = vector_db.similarity_search(user_query, k=3)
    context = "\n".join([d.page_content for d in docs])

    # SUY LU·∫¨N: G·ªçi b·ªô n√£o AI local
    answer = call_local_ai(user_query, context)

    # T·ª∞ H·ªåC: L∆∞u c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi m·ªõi v√†o b·ªô nh·ªõ vƒ©nh vi·ªÖn
    new_knowledge = f"Q: {user_query} | A: {answer}"
    vector_db.add_documents([Document(page_content=new_knowledge)])
    vector_db.persist()

    return jsonify({
        "status": "success",
        "answer": answer,
        "memory_updated": True
    })

if __name__ == "__main__":
    print("üöÄ EyeSpyhub AI Server is running locally on port 5000...")
    app.run(port=5000)
